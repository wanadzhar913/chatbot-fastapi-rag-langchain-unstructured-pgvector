{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why LangChain?\n",
    "- Modularity: Composable building blocks, tools and integrations for working with language models.\n",
    "- Ease of Integration: Using Mistral instead of OpenAI is just a `Class` change away instead of having to learn an entirely new API.\n",
    "- Enhanced Capabilities: LangChain Expression Language and Runnable interface.\n",
    "- Community and Support: It's got the largest community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import (\n",
    "    create_tool_calling_agent,\n",
    "    AgentExecutor,\n",
    ")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.runnables import (\n",
    "    RunnablePassthrough,\n",
    "    RunnableParallel,\n",
    ")\n",
    "\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.chat_message_histories import (\n",
    "    ChatMessageHistory,\n",
    "    RedisChatMessageHistory,\n",
    ")\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.0 The Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} # because our prompt expects 'topic' as input\n",
    "    | prompt # the pipes work just like bash scripting!\n",
    "    | model # they're a small part called LangChain Expression Language\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the parrot wear a raincoat? Because he wanted to be poly-unsaturated!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the most standard method.\n",
    "chain.invoke(\"a parrot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at the **Runnable Interface** (Stream, Invoke, Batch, Asynchronous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the parrot wear a raincoat?\n",
      "\n",
      "Because he wanted to be polyunsaturated!"
     ]
    }
   ],
   "source": [
    "# use LangChain's stream API (so we print out token by token)\n",
    "for s in chain.stream(\"a parrot\"):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the parrot wear a raincoat?\\n\\nBecause he wanted to be poly-unsaturated!',\n",
       " 'Why did the weather go to therapy? It had too many mood swings!',\n",
       " 'Why did Timothee Chalamet bring a ladder to the bar? \\n\\nBecause he heard the drinks were on a higher level!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch requests\n",
    "chain.batch(['a parrot', 'the weather', 'Timothee Chalamet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the parrot wear a raincoat?\n",
      "\n",
      "Because he wanted to be poly-\"unsaturated\"!"
     ]
    }
   ],
   "source": [
    "# asynchronous\n",
    "async for s in chain.astream(\"a parrot\"):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the parrot wear a raincoat?\\n\\nBecause he wanted to be polyunsaturated!'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asynchronous\n",
    "await chain.ainvoke(\"a parrot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Why did the parrot wear a raincoat?\\n\\nBecause it wanted to be polyunsaturated!',\n",
       " 'Why did the snowman bring a scarf to the beach?\\nBecause he heard it was going to be a little chili!',\n",
       " 'Why did Timothee Chalamet bring a ladder to the red carpet? \\n\\nBecause he heard the paparazzi were always looking for a higher angle to photograph him from!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# asynchronous\n",
    "await chain.abatch(['a parrot', 'the weather', 'Timothee Chalamet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.0 Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_session_history,\n",
    "    input_messages_key=\"topic\",\n",
    "    history_messages_key='history',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear dissolve in water?\\n\\nBecause it was a polar bear!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke(\n",
    "    {\"topic\": \"bears\"},\n",
    "    {\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': InMemoryChatMessageHistory(messages=[HumanMessage(content='bears', additional_kwargs={}, response_metadata={}), AIMessage(content='Why did the bear dissolve in water?\\n\\nBecause it was a polar bear!', additional_kwargs={}, response_metadata={})])}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the bear wear a fur coat in the summer? \\n\\nBecause he wanted to be bear-y stylish!'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since out session id is the same, it should make a joke about bears\n",
    "with_message_history.invoke(\n",
    "    {\"topic\": \"Make a joke about an animal I just asked about\"},\n",
    "    {\"configurable\": {\"session_id\": \"1\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': InMemoryChatMessageHistory(messages=[HumanMessage(content='bears', additional_kwargs={}, response_metadata={}), AIMessage(content='Why did the bear dissolve in water?\\n\\nBecause it was a polar bear!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Make a joke about an animal I just asked about', additional_kwargs={}, response_metadata={}), AIMessage(content='Why did the bear dissolve in water? Because it was a polar bear!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Make a joke about an animal I just asked about', additional_kwargs={}, response_metadata={}), AIMessage(content='Why did the bear wear a fur coat in the summer? \\n\\nBecause he wanted to be bear-y stylish!', additional_kwargs={}, response_metadata={})])}\n"
     ]
    }
   ],
   "source": [
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an alternative to an in-memory chat history storage solution. You can use Redis instead and still use `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REDIS_URL = \"redis://localhost:6379\"\n",
    "\n",
    "# def get_message_history(session_id: str) -> RedisChatMessageHistory:\n",
    "#     return RedisChatMessageHistory(session_id, url=REDIS_URL)\n",
    "\n",
    "# with_message_history = RunnableWithMessageHistory(\n",
    "#     chain,\n",
    "#     get_session_history=get_message_history,\n",
    "#     input_messages_key=\"topic\",\n",
    "#     history_messages_key='history',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.0 RAG (Retrieval Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read PDF\n",
    "pdf_reader = PyPDFLoader(\"../documents/Alexiou and Kartiyasa (2019).pdf\")\n",
    "\n",
    "pages = []\n",
    "async for page in pdf_reader.alazy_load():\n",
    "    pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': None,\n",
       " 'metadata': {'source': '../documents/Alexiou and Kartiyasa (2019).pdf',\n",
       "  'page': 0},\n",
       " 'page_content': 'Received: 6 March 2019 Revised: 15 September 2019 Accepted: 27 November 2019\\nDOI: 10.1111/boer.12226\\nARTICLE\\nDoes greater income inequality cause increased work\\nhours? New evidence from high income economies\\nConstantinos Alexiou Adimulya Kartiyasa\\nCranfield University, College Road, UK\\nCorrespondence\\nConstantinos Alexiou, Cranfield University,\\nCollege Road, Bedfordshire, MK430AL\\nCranfield, UK.\\nEmail: constantinos.alexiou@cranfield.ac.uk\\nAdimulya Kartiyasa, Cranfield University,\\nCollege Road, Bedfordshire, MK430AL\\nCranfield, UK.\\nEmail: A.Kartiyasa@cranfield.ac.uk\\nAbstract\\nWe explore the extent to which individual’s allocation of\\ntime between labour and leisure is affected by the consump-\\ntion standards of the rich. Utilizing a panel data method-\\nology and panel Granger causality tests we investigate the\\nrelationship between income inequality and work hours\\nfor a cluster of 24 high-income OECD countries over the\\nperiod 1990–2015. Four alternative measures of income\\ninequality are considered. We find that greater income\\ninequality is associated with longer work hours indicat-\\ning stronger concern for conspicuous consumption rather\\nthan conspicuous leisure. Even though the resulting esti-\\nmates lend support to the theoretical framework on con-\\nsumption emulation, the generated evidence also appears\\nto be in line with a Duesenberry’s and Frank’s expendi-\\nture cascading approach. The ambiguity however arising\\nfrom the Granger Causality tests appears to lead – to a cer-\\ntain extent – to different conclusions about the direction of\\ncausality or whether a causal relationship does even exist.\\nIt is therefore imperative that caution should be exercised\\nwhen interpreting the direction of the causal dimension.\\nKEYWORDS\\ncausality, income inequality, panel data, Veblen effect, work hours\\nJEL CLASSIFICATION\\nC2, D63, J2\\n© 2020 Board of Trustees of the Bulletin of Economic Research and John Wiley & Sons Ltd\\n380 wileyonlinelibrary.com/journal/boer Bull Econ Res.2020;72:380–392.\\n',\n",
       " 'type': 'Document'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../documents/Alexiou and Kartiyasa (2019).pdf', 'page': 0}\n",
      "\n",
      "Received: 6 March 2019 Revised: 15 September 2019 Accepted: 27 November 2019\n",
      "DOI: 10.1111/boer.12226\n",
      "ARTICLE\n",
      "Does greater income inequality cause increased work\n",
      "hours? New evidence from high income economies\n",
      "Constantinos Alexiou Adimulya Kartiyasa\n",
      "Cranfield University, College Road, UK\n",
      "Correspondence\n",
      "Constantinos Alexiou, Cranfield University,\n",
      "College Road, Bedfordshire, MK430AL\n",
      "Cranfield, UK.\n",
      "Email: constantinos.alexiou@cranfield.ac.uk\n",
      "Adimulya Kartiyasa, Cranfield University,\n",
      "College Road, Bedfordshire, MK430AL\n",
      "Cranfield, UK.\n",
      "Email: A.Kartiyasa@cranfield.ac.uk\n",
      "Abstract\n",
      "We explore the extent to which individual’s allocation of\n",
      "time between labour and leisure is affected by the consump-\n",
      "tion standards of the rich. Utilizing a panel data method-\n",
      "ology and panel Granger causality tests we investigate the\n",
      "relationship between income inequality and work hours\n",
      "for a cluster of 24 high-income OECD countries over the\n",
      "period 1990–2015. Four alternative measures of income\n",
      "inequality are considered. We find that greater income\n",
      "inequality is associated with longer work hours indicat-\n",
      "ing stronger concern for conspicuous consumption rather\n",
      "than conspicuous leisure. Even though the resulting esti-\n",
      "mates lend support to the theoretical framework on con-\n",
      "sumption emulation, the generated evidence also appears\n",
      "to be in line with a Duesenberry’s and Frank’s expendi-\n",
      "ture cascading approach. The ambiguity however arising\n",
      "from the Granger Causality tests appears to lead – to a cer-\n",
      "tain extent – to different conclusions about the direction of\n",
      "causality or whether a causal relationship does even exist.\n",
      "It is therefore imperative that caution should be exercised\n",
      "when interpreting the direction of the causal dimension.\n",
      "KEYWORDS\n",
      "causality, income inequality, panel data, Veblen effect, work hours\n",
      "JEL CLASSIFICATION\n",
      "C2, D63, J2\n",
      "© 2020 Board of Trustees of the Bulletin of Economic Research and John Wiley & Sons Ltd\n",
      "380 wileyonlinelibrary.com/journal/boer Bull Econ Res.2020;72:380–392.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pages[0].metadata}\\n\")\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings before loading into a vector store/knowledge base\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "# we'll use FAISS as our vector store.\n",
    "vector_store = FAISS.from_documents(pages, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "        # \"score_threshold\": 0.1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "You are an assistant for question-answering tasks. Use the following pieces of\n",
    "retrieved context to answer the question. If you don't know the answer, just\n",
    "say you don't know. Use three sentences maximum and keep the answer concise.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Function to only get `page_content` data from PyPDFLoader\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "rag_chain = (\n",
    "    # RunnablePassThrough just gives the output as is\n",
    "    RunnableParallel({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()})\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bell and Freeman (2001) reported that workers in the European Community prefer pay increases to decreases in hours, suggesting that current work hours are close to their ideal choice. This indicates that the relationship between inequality and work hours is influenced by individual preferences and perceived ideal working conditions. Additionally, similar results were found in a study by Böheim and Taylor (2004) using the British Household Panel Survey.'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What did Bell and Freeman (2001) report?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.0 Tool Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1 Tool Calling Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the below, **you must provide the docstring.** It's for the LLM to identify if it needs to use the tool or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def fake_weather_api(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Check the weather in a specified city.\n",
    "\n",
    "    Args:\n",
    "    - `city` (str): The name of the city where you want to check the weather.\n",
    "\n",
    "    Returns:\n",
    "    - (str): A description of the current weather in the specified city.\n",
    "    \"\"\"\n",
    "    return \"Sunny, 22°C\"\n",
    "\n",
    "tools = [fake_weather_api]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\")\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_NVK9urq9Sv5PsXt1YK0OzHe5', 'function': {'arguments': '{\"city\":\"Munich\"}', 'name': 'fake_weather_api'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 101, 'total_tokens': 118, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-6f0dddb8-774e-444c-bdfb-c717acf675d4-0', tool_calls=[{'name': 'fake_weather_api', 'args': {'city': 'Munich'}, 'id': 'call_NVK9urq9Sv5PsXt1YK0OzHe5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 101, 'output_tokens': 17, 'total_tokens': 118, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you'll notice that content is empty!\n",
    "result = llm_with_tools.invoke(\"How will the weather be in Munich today?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'call_NVK9urq9Sv5PsXt1YK0OzHe5',\n",
       "    'function': {'arguments': '{\"city\":\"Munich\"}', 'name': 'fake_weather_api'},\n",
       "    'type': 'function'}],\n",
       "  'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 17,\n",
       "   'prompt_tokens': 101,\n",
       "   'total_tokens': 118,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-3.5-turbo-0125',\n",
       "  'system_fingerprint': None,\n",
       "  'finish_reason': 'tool_calls',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-6f0dddb8-774e-444c-bdfb-c717acf675d4-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'fake_weather_api',\n",
       "   'args': {'city': 'Munich'},\n",
       "   'id': 'call_NVK9urq9Sv5PsXt1YK0OzHe5',\n",
       "   'type': 'tool_call'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 101,\n",
       "  'output_tokens': 17,\n",
       "  'total_tokens': 118,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(\n",
    "        \"How will the weather be in Munich today? I would like to eat outside if possible.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "llm_output = llm_with_tools.invoke(messages)\n",
    "messages.append(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='How will the weather be in Munich today? I would like to eat outside if possible.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7tOMJ2uuBJODOqtJo8S5tybB', 'function': {'arguments': '{\"city\":\"Munich\"}', 'name': 'fake_weather_api'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 110, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4d879044-3466-424b-ae07-4cf35abaed6e-0', tool_calls=[{'name': 'fake_weather_api', 'args': {'city': 'Munich'}, 'id': 'call_7tOMJ2uuBJODOqtJo8S5tybB', 'type': 'tool_call'}], usage_metadata={'input_tokens': 110, 'output_tokens': 17, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fake_weather_api': StructuredTool(name='fake_weather_api', description='Check the weather in a specified city.\\n\\nArgs:\\n- `city` (str): The name of the city where you want to check the weather.\\n\\nReturns:\\n- (str): A description of the current weather in the specified city.', args_schema=<class 'langchain_core.utils.pydantic.fake_weather_api'>, func=<function fake_weather_api at 0x7fa869ced8a0>)}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_mapping = {\n",
    "    \"fake_weather_api\": fake_weather_api\n",
    "}\n",
    "\n",
    "tool_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in llm_output.tool_calls:\n",
    "    tool = tool_mapping[tool_call[\"name\"].lower()]\n",
    "    tool_output = tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='How will the weather be in Munich today? I would like to eat outside if possible.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_7tOMJ2uuBJODOqtJo8S5tybB', 'function': {'arguments': '{\"city\":\"Munich\"}', 'name': 'fake_weather_api'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 110, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-4d879044-3466-424b-ae07-4cf35abaed6e-0', tool_calls=[{'name': 'fake_weather_api', 'args': {'city': 'Munich'}, 'id': 'call_7tOMJ2uuBJODOqtJo8S5tybB', 'type': 'tool_call'}], usage_metadata={'input_tokens': 110, 'output_tokens': 17, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       " ToolMessage(content='Sunny, 22°C', tool_call_id='call_7tOMJ2uuBJODOqtJo8S5tybB')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The weather in Munich today is sunny with a temperature of 22°C. It sounds like a perfect day to eat outside! Enjoy your meal!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 142, 'total_tokens': 172, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-e29b88de-28d1-49d2-a07d-0a5cc5472f19-0', usage_metadata={'input_tokens': 142, 'output_tokens': 30, 'total_tokens': 172, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.2 Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents are able to use tools autonomously and can reflect and use their own thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tool #1\n",
    "@tool\n",
    "def fake_weather_api(city: str) -> str:\n",
    "    \"\"\"\n",
    "    Check the weather in a specified city.\n",
    "\n",
    "    Args:\n",
    "    - `city` (str): The name of the city where you want to check the weather.\n",
    "\n",
    "    Returns:\n",
    "    - (str): A description of the current weather in the specified city.\n",
    "    \"\"\"\n",
    "    return \"Sunny, 22°C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "\n",
    "for docs in [\n",
    "    \"../documents/Alexiou and Kartiyasa (2019).pdf\",\n",
    "    \"../documents/Bell and Freeman (2001).pdf\"\n",
    "]:\n",
    "    pdf_reader = PyPDFLoader(docs)\n",
    "\n",
    "    async for page in pdf_reader.alazy_load():\n",
    "        pages.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings before loading into a vector store/knowledge base\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\")\n",
    "\n",
    "# we'll use FAISS as our vector store.\n",
    "vector_store = FAISS.from_documents(pages, embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 3,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tool #2\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"academic_paper_retriever\",\n",
    "    \"Get information about papers studying work hours and inequality\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [fake_weather_api, retriever_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faiq0913/Personal-Projects/chatbot-fastapi-rag-langchain-unstructured-pgvector/.venv/lib/python3.11/site-packages/langsmith/client.py:261: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fa8b22171a0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fa8b22171a0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a specialized prompt from LangChain Hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_tool_calling_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What does Park and Bowles say about work hours?',\n",
       " 'output': 'Park and Bowles, in their study, discuss the relationship between income inequality and work hours. They suggest that pay inequality serves as an indicator of incentives in a labor supply equation, with wider earnings distribution leading to greater work hours and effort by workers to improve their position in the earnings distribution. The study highlights empirical evidence from Lazear and Rosen\\'s tournament model, indicating that wage inequality influences how much individuals are willing to work. Additionally, the study references Veblen\\'s and Duesenberry\\'s empirical studies, showing that consumption patterns and income inequality significantly impact individual consumption and working patterns.\\n\\nFurthermore, the study by Bell and Freeman demonstrates a positive relationship between earnings inequality and work hours in the US and Germany. It suggests that hours worked can affect future wages and promotion prospects. Osberg\\'s study contrasts the differences in average working hours between Germany and the USA, attributing them to differences in workforce participation and collective decisions like longer vacations and holidays.\\n\\nPark and Bowles also discuss the \"Veblen effect,\" where individuals desire to emulate the consumption standards of the rich, leading to decisions to work longer hours instead of leisure. The study by Oh, Park, and Bowles analyzes the downward trend in working hours in developed countries and finds correlations between top 1% income share, political representation, and work hours.\\n\\nOverall, the research indicates that income inequality and social comparisons play a significant role in determining work hours and consumption patterns among individuals. The implications of these findings for policymakers suggest the need for tailored policies to address market failures related to Veblen effects and income distribution.'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use case where we ask a question about academic papers\n",
    "agent_executor.invoke({\"input\": \"What does Park and Bowles say about work hours?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': \"What's the weather in Kuala Lumpur?\",\n",
       " 'output': 'The current weather in Kuala Lumpur is sunny with a temperature of 22°C.'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use case where we ask a question about the weather\n",
    "agent_executor.invoke({\"input\": \"What's the weather in Kuala Lumpur?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
